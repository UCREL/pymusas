"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[13],{3905:function(e,t,n){n.d(t,{Zo:function(){return c},kt:function(){return u}});var a=n(7294);function o(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function s(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function r(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?s(Object(n),!0).forEach((function(t){o(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):s(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function i(e,t){if(null==e)return{};var n,a,o=function(e,t){if(null==e)return{};var n,a,o={},s=Object.keys(e);for(a=0;a<s.length;a++)n=s[a],t.indexOf(n)>=0||(o[n]=e[n]);return o}(e,t);if(Object.getOwnPropertySymbols){var s=Object.getOwnPropertySymbols(e);for(a=0;a<s.length;a++)n=s[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var l=a.createContext({}),p=function(e){var t=a.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):r(r({},t),e)),n},c=function(e){var t=p(e.components);return a.createElement(l.Provider,{value:t},e.children)},d={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},m=a.forwardRef((function(e,t){var n=e.components,o=e.mdxType,s=e.originalType,l=e.parentName,c=i(e,["components","mdxType","originalType","parentName"]),m=p(n),u=o,h=m["".concat(l,".").concat(u)]||m[u]||d[u]||s;return n?a.createElement(h,r(r({ref:t},c),{},{components:n})):a.createElement(h,r({ref:t},c))}));function u(e,t){var n=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var s=n.length,r=new Array(s);r[0]=m;var i={};for(var l in t)hasOwnProperty.call(t,l)&&(i[l]=t[l]);i.originalType=e,i.mdxType="string"==typeof e?e:o,r[1]=i;for(var p=2;p<s;p++)r[p]=n[p];return a.createElement.apply(null,r)}return a.createElement.apply(null,n)}m.displayName="MDXCreateElement"},9710:function(e,t,n){n.r(t),n.d(t,{frontMatter:function(){return i},contentTitle:function(){return l},metadata:function(){return p},toc:function(){return c},default:function(){return m}});var a=n(3117),o=n(102),s=(n(7294),n(3905)),r=["components"],i={title:"Tag CoNLL-U Files",sidebar_position:2},l=void 0,p={unversionedId:"usage/how_to/tag_conll_u_files",id:"usage/how_to/tag_conll_u_files",title:"Tag CoNLL-U Files",description:"In this guide we will show how to tag text that is in CoNLL-U formatted files and save the output to a TSV file. To make this guide as simple as possible we are going to base it on one language, French, and we are going to tag the French GSD Universal Dependencies version 2.10 development treebank. As we are tagging a treebank that contains gold standard tokens, lemmas, and Part Of Speech tags we will not need any other NLP tools other than the rule based pre-configured French PyMUSAS spaCy pipeline which will output USAS semantic tags.",source:"@site/docs/usage/how_to/tag_conll_u_files.md",sourceDirName:"usage/how_to",slug:"/usage/how_to/tag_conll_u_files",permalink:"/pymusas/usage/how_to/tag_conll_u_files",editUrl:"https://github.com/ucrel/pymusas/edit/main/docs/docs/usage/how_to/tag_conll_u_files.md",tags:[],version:"current",lastUpdatedBy:"Daisy Lal",lastUpdatedAt:1692311686,formattedLastUpdatedAt:"8/17/2023",sidebarPosition:2,frontMatter:{title:"Tag CoNLL-U Files",sidebar_position:2},sidebar:"docs",previous:{title:"Tag Text",permalink:"/pymusas/usage/how_to/tag_text"},next:{title:"Multi Word Expression Syntax",permalink:"/pymusas/usage/notes/mwe_syntax"}},c=[{value:"Download the data",id:"download-the-data",children:[],level:2},{value:"Python requirements",id:"python-requirements",children:[],level:2},{value:"Tag a CoNLL-U File",id:"tag-a-conll-u-file",children:[],level:2}],d={toc:c};function m(e){var t=e.components,n=(0,o.Z)(e,r);return(0,s.kt)("wrapper",(0,a.Z)({},d,n,{components:t,mdxType:"MDXLayout"}),(0,s.kt)("p",null,"In this guide we will show how to tag text that is in ",(0,s.kt)("a",{parentName:"p",href:"https://universaldependencies.org/format.html"},"CoNLL-U formatted files")," and save the output to a TSV file. To make this guide as simple as possible we are going to base it on one language, ",(0,s.kt)("strong",{parentName:"p"},"French"),", and we are going to tag the ",(0,s.kt)("a",{parentName:"p",href:"https://raw.githubusercontent.com/UniversalDependencies/UD_French-GSD/r2.10/fr_gsd-ud-dev.conllu"},"French GSD Universal Dependencies version 2.10 development treebank"),". As we are tagging a treebank that contains gold standard tokens, lemmas, and Part Of Speech tags we will not need any other NLP tools other than the ",(0,s.kt)("a",{parentName:"p",href:"https://github.com/UCREL/pymusas-models/releases/tag/fr_single_upos2usas_contextual-0.3.1"},"rule based pre-configured French PyMUSAS spaCy pipeline")," which will output ",(0,s.kt)("a",{parentName:"p",href:"https://ucrel.lancs.ac.uk/usas/"},"USAS")," semantic tags."),(0,s.kt)("h2",{id:"download-the-data"},"Download the data"),(0,s.kt)("p",null,"First we are going to download the ",(0,s.kt)("a",{parentName:"p",href:"https://raw.githubusercontent.com/UniversalDependencies/UD_French-GSD/r2.10/fr_gsd-ud-dev.conllu"},"French GSD Universal Dependencies version 2.10 development treebank")," and save it as ",(0,s.kt)("inlineCode",{parentName:"p"},"french_gsd_dev_2_10.conllu")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-bash"},"curl -o french_gsd_dev_2_10.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_French-GSD/r2.10/fr_gsd-ud-dev.conllu\n")),(0,s.kt)("p",null,"The first 5 lines of the file should contain the following:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-txt",metastring:'title="french_gsd_dev_2_10.conllu"',title:'"french_gsd_dev_2_10.conllu"'},"# global.columns = ID FORM LEMMA UPOS XPOS FEATS HEAD DEPREL DEPS MISC\n# sent_id = fr-ud-dev_00001\n# text = Aviator, un film sur la vie de Hughes.\n1   Aviator Aviator PROPN   _   _   0   root    _   SpaceAfter=No\n2   ,   ,   PUNCT   _   _   4   punct   _   _\n")),(0,s.kt)("h2",{id:"python-requirements"},"Python requirements"),(0,s.kt)("div",{className:"admonition admonition-note alert alert--secondary"},(0,s.kt)("div",{parentName:"div",className:"admonition-heading"},(0,s.kt)("h5",{parentName:"div"},(0,s.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,s.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,s.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"}))),"note")),(0,s.kt)("div",{parentName:"div",className:"admonition-content"},(0,s.kt)("p",{parentName:"div"},"We assume for this guide that you have Python version >= ",(0,s.kt)("inlineCode",{parentName:"p"},"3.7")," already installed."))),(0,s.kt)("p",null,"Now that we have the data we need to download the ",(0,s.kt)("a",{parentName:"p",href:"https://github.com/UCREL/pymusas-models/releases/tag/fr_single_upos2usas_contextual-0.3.1"},"French PyMUSAS spaCy pipeline"),":"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-bash"},"pip install https://github.com/UCREL/pymusas-models/releases/download/fr_single_upos2usas_contextual-0.3.1/fr_single_upos2usas_contextual-0.3.1-py3-none-any.whl\n")),(0,s.kt)("p",null,"And to easily parse the ConLL-U file we are going to use the Python package ",(0,s.kt)("a",{parentName:"p",href:"https://pypi.org/project/conllu/"},"conllu"),":"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-bash"},"pip install conllu==4.4.2\n")),(0,s.kt)("h2",{id:"tag-a-conll-u-file"},"Tag a CoNLL-U File"),(0,s.kt)("p",null,"First we are going to load the French PyMUSAS spaCy pipeline:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},'import csv\nfrom pathlib import Path\nfrom typing import List\n\nfrom conllu import parse_incr\nimport spacy\n\n# Load the French PyMUSAS rule based tagger\nnlp = spacy.load("fr_single_upos2usas_contextual")\n')),(0,s.kt)("p",null,"Then we are going to create the TSV file, ",(0,s.kt)("inlineCode",{parentName:"p"},"french_gsd_dev_2_10.tsv"),", and the TSV writer so that we can save the tagged:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"french_gsd_file = Path('french_gsd_dev_2_10.conllu').resolve()\nfrench_output_tsv_file = Path('french_gsd_dev_2_10.tsv').resolve()\n\nwith french_output_tsv_file.open('w', encoding='utf-8', newline='') as output_fp:\n    field_names = ['Token', 'Lemma', 'UPOS', 'USAS Tags']\n    tsv_writer = csv.DictWriter(output_fp, fieldnames=field_names, delimiter='\\t')\n    tsv_writer.writeheader()\n")),(0,s.kt)("p",null,"Now we are going to read each sentence in the French treebank and extract out the token, lemma, Universal Part Of Speech (UPOS) tag, and if the token contains a space after it or not (default is that there is a space after each token):"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"    with french_gsd_file.open('r', encoding='utf-8') as french_gsd_fp:\n        for sentence in parse_incr(french_gsd_fp):\n            sentence = sentence.filter(id=lambda x: type(x) is int)\n            token_text: List[str] = []\n            spaces: List[bool] = []\n            lemmas: List[str] = []\n            upos_tags: List[str] = []\n            \n            for token_data in sentence:\n                token_text.append(token_data['form'])\n                lemmas.append(token_data['lemma'])\n                upos_tags.append(token_data['upos'])\n                space = True\n                if isinstance(token_data['misc'], dict):\n                    if token_data['misc'].get('SpaceAfter', 'yes').lower() == 'no':\n                        space = False\n                spaces.append(space)\n")),(0,s.kt)("div",{className:"admonition admonition-note alert alert--secondary"},(0,s.kt)("div",{parentName:"div",className:"admonition-heading"},(0,s.kt)("h5",{parentName:"div"},(0,s.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,s.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,s.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"}))),"note")),(0,s.kt)("div",{parentName:"div",className:"admonition-content"},(0,s.kt)("p",{parentName:"div"},"Notice that we add the filter:"),(0,s.kt)("pre",{parentName:"div"},(0,s.kt)("code",{parentName:"pre",className:"language-python"},"sentence = sentence.filter(id=lambda x: type(x) is int)\n")),(0,s.kt)("p",{parentName:"div"},"This removes all tokens that have an ",(0,s.kt)("inlineCode",{parentName:"p"},"id")," value that is not an integer, in CoNLL-U format the two token types that do not have integer values are: "),(0,s.kt)("ol",{parentName:"div"},(0,s.kt)("li",{parentName:"ol"},"Tokens that get broken down into multiword tokens, these tokens are represented with an ",(0,s.kt)("inlineCode",{parentName:"li"},"id")," that has an integer range, e.g. ",(0,s.kt)("inlineCode",{parentName:"li"},"1-2"),"."),(0,s.kt)("li",{parentName:"ol"},"Empty nodes, these tokens are represented with an ",(0,s.kt)("inlineCode",{parentName:"li"},"id")," that is a float, e.g. ",(0,s.kt)("inlineCode",{parentName:"li"},"1.1"),".")),(0,s.kt)("p",{parentName:"div"},"For more information on this see the ",(0,s.kt)("a",{parentName:"p",href:"https://universaldependencies.org/format.html#words-tokens-and-empty-nodes"},"words, tokens and empty nodes section of the CoNLL-U format page.")))),(0,s.kt)("div",{className:"admonition admonition-note alert alert--secondary"},(0,s.kt)("div",{parentName:"div",className:"admonition-heading"},(0,s.kt)("h5",{parentName:"div"},(0,s.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,s.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,s.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"}))),"note")),(0,s.kt)("div",{parentName:"div",className:"admonition-content"},(0,s.kt)("p",{parentName:"div"},"In this example we use the UPOS tag for the POS tag, but some languages like ",(0,s.kt)("a",{parentName:"p",href:"https://ucrel.github.io/pymusas/usage/how_to/tag_text#welsh"},"Welsh")," require the POS tag to be from a different POS tagset other than the UPOS tagset that the UPOS tag comes from."),(0,s.kt)("p",{parentName:"div"},"To know which POS tagset a PyMUSAS model requires the POS data to come from see the ",(0,s.kt)("a",{parentName:"p",href:"https://github.com/UCREL/pymusas-models#overview-of-the-models"},"overview of the models table from the PyMUSAS models repository"),"."))),(0,s.kt)("p",null,"Once we have this data for a sentence we shall use it to semantically tag the text:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"            # As the tagger is a spaCy pipeline that expects tokens, pos, and lemma\n            # we need to create a spaCy Doc object that will contain this information\n            doc = spacy.tokens.Doc(nlp.vocab, words=token_text, pos=upos_tags,\n                                   lemmas=lemmas, spaces=spaces)\n            output_doc = nlp(doc)\n")),(0,s.kt)("p",null,"Now we have the token, lemma, UPOS, and USAS semantic tag stored in the spaCy ",(0,s.kt)("inlineCode",{parentName:"p"},"doc")," object we can then save it to a TSV file like so:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"            # Write to TSV file\n            for token in output_doc:\n                tsv_writer.writerow({'Token': token.text,\n                                     'Lemma': token.lemma_,\n                                     'UPOS': token.pos_,\n                                     'USAS Tags': token._.pymusas_tags})\n")),(0,s.kt)("p",null,"The first 5 lines of the TSV output file should contain the following:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-tsv",metastring:'title="french_gsd_dev_2_10.tsv"',title:'"french_gsd_dev_2_10.tsv"'},"Token   Lemma   UPOS    USAS Tags\nAviator Aviator PROPN   ['Z99']\n,   ,   PUNCT   ['PUNCT']\nun  un  DET ['Z5']\nfilm    film    NOUN    ['Q4.3', 'O2/C1', 'O1.1', 'O1.2', 'B3']\n")),(0,s.kt)("p",null,"The full Python script for this example can be seen below:"),(0,s.kt)("details",null,(0,s.kt)("summary",null,"Python Script"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"import csv\nfrom pathlib import Path\nfrom typing import List\n\nfrom conllu import parse_incr\nimport spacy\n\n# Load the French PyMUSAS rule based tagger\nnlp = spacy.load(\"fr_single_upos2usas_contextual\")\n\nfrench_gsd_file = Path('french_gsd_dev_2_10.conllu').resolve()\nfrench_output_tsv_file = Path('french_gsd_dev_2_10.tsv').resolve()\n\nwith french_output_tsv_file.open('w', encoding='utf-8', newline='') as output_fp:\n    field_names = ['Token', 'Lemma', 'UPOS', 'USAS Tags']\n    tsv_writer = csv.DictWriter(output_fp, fieldnames=field_names, delimiter='\\t')\n    tsv_writer.writeheader()\n\n    with french_gsd_file.open('r', encoding='utf-8') as french_gsd_fp:\n        for sentence in parse_incr(french_gsd_fp):\n            sentence = sentence.filter(id=lambda x: type(x) is int)\n            token_text: List[str] = []\n            spaces: List[bool] = []\n            lemmas: List[str] = []\n            upos_tags: List[str] = []\n            \n            for token_data in sentence:\n                token_text.append(token_data['form'])\n                lemmas.append(token_data['lemma'])\n                upos_tags.append(token_data['upos'])\n                space = True\n                if isinstance(token_data['misc'], dict):\n                    if token_data['misc'].get('SpaceAfter', 'yes').lower() == 'no':\n                        space = False\n                spaces.append(space)\n\n            # As the tagger is a spaCy pipeline that expects tokens, pos, and lemma\n            # we need to create a spaCy Doc object that will contain this information\n            doc = spacy.tokens.Doc(nlp.vocab, words=token_text, pos=upos_tags,\n                                lemmas=lemmas, spaces=spaces)\n            output_doc = nlp(doc)\n\n            # Write to TSV file\n            for token in output_doc:\n                tsv_writer.writerow({'Token': token.text,\n                                     'Lemma': token.lemma_,\n                                     'UPOS': token.pos_,\n                                     'USAS Tags': token._.pymusas_tags})\n"))))}m.isMDXComponent=!0}}]);