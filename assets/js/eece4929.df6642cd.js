"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[15],{3905:function(e,n,t){t.d(n,{Zo:function(){return d},kt:function(){return c}});var a=t(7294);function s(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function o(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);n&&(a=a.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,a)}return t}function l(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?o(Object(t),!0).forEach((function(n){s(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):o(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function i(e,n){if(null==e)return{};var t,a,s=function(e,n){if(null==e)return{};var t,a,s={},o=Object.keys(e);for(a=0;a<o.length;a++)t=o[a],n.indexOf(t)>=0||(s[t]=e[t]);return s}(e,n);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)t=o[a],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(s[t]=e[t])}return s}var r=a.createContext({}),p=function(e){var n=a.useContext(r),t=n;return e&&(t="function"==typeof e?e(n):l(l({},n),e)),t},d=function(e){var n=p(e.components);return a.createElement(r.Provider,{value:n},e.children)},u={inlineCode:"code",wrapper:function(e){var n=e.children;return a.createElement(a.Fragment,{},n)}},m=a.forwardRef((function(e,n){var t=e.components,s=e.mdxType,o=e.originalType,r=e.parentName,d=i(e,["components","mdxType","originalType","parentName"]),m=p(t),c=s,g=m["".concat(r,".").concat(c)]||m[c]||u[c]||o;return t?a.createElement(g,l(l({ref:n},d),{},{components:t})):a.createElement(g,l({ref:n},d))}));function c(e,n){var t=arguments,s=n&&n.mdxType;if("string"==typeof e||s){var o=t.length,l=new Array(o);l[0]=m;var i={};for(var r in n)hasOwnProperty.call(n,r)&&(i[r]=n[r]);i.originalType=e,i.mdxType="string"==typeof e?e:s,l[1]=i;for(var p=2;p<o;p++)l[p]=t[p];return a.createElement.apply(null,l)}return a.createElement.apply(null,t)}m.displayName="MDXCreateElement"},9369:function(e,n,t){t.r(n),t.d(n,{frontMatter:function(){return i},contentTitle:function(){return r},metadata:function(){return p},toc:function(){return d},default:function(){return m}});var a=t(3117),s=t(102),o=(t(7294),t(3905)),l=["components"],i={title:"Tag Text",sidebar_position:1},r=void 0,p={unversionedId:"usage/how_to/tag_text",id:"usage/how_to/tag_text",title:"Tag Text",description:"In this guide we are going to show you how to tag text using the PyMUSAS RuleBasedTagger so that you can extract token level USAS semantic tags from the tagged text. The guide is broken down into different languages, for each guide we are going to:",source:"@site/docs/usage/how_to/tag_text.md",sourceDirName:"usage/how_to",slug:"/usage/how_to/tag_text",permalink:"/pymusas/usage/how_to/tag_text",editUrl:"https://github.com/ucrel/pymusas/edit/main/docs/docs/usage/how_to/tag_text.md",tags:[],version:"current",lastUpdatedBy:"Paul Rayson",lastUpdatedAt:1651672020,formattedLastUpdatedAt:"5/4/2022",sidebarPosition:1,frontMatter:{title:"Tag Text",sidebar_position:1},sidebar:"docs",previous:{title:"Installation",permalink:"/pymusas/installation"},next:{title:"Multi Word Expression Syntax",permalink:"/pymusas/usage/notes/mwe_syntax"}},d=[{value:"Chinese",id:"chinese",children:[],level:2},{value:"Dutch",id:"dutch",children:[],level:2},{value:"French",id:"french",children:[],level:2},{value:"Italian",id:"italian",children:[],level:2},{value:"Portuguese",id:"portuguese",children:[],level:2},{value:"Spanish",id:"spanish",children:[],level:2},{value:"Welsh",id:"welsh",children:[],level:2},{value:"Indonesian",id:"indonesian",children:[],level:2}],u={toc:d};function m(e){var n=e.components,t=(0,s.Z)(e,l);return(0,o.kt)("wrapper",(0,a.Z)({},u,t,{components:n,mdxType:"MDXLayout"}),(0,o.kt)("p",null,"In this guide we are going to show you how to tag text using the PyMUSAS ",(0,o.kt)("a",{parentName:"p",href:"/api/spacy_api/taggers/rule_based#rulebasedtagger"},"RuleBasedTagger")," so that you can extract token level ",(0,o.kt)("a",{parentName:"p",href:"https://ucrel.lancs.ac.uk/usas/"},"USAS semantic tags")," from the tagged text. The guide is broken down into different languages, for each guide we are going to: "),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},"Download the relevant pre-configured PyMUSAS ",(0,o.kt)("inlineCode",{parentName:"li"},"RuleBasedTagger")," spaCy component for the language."),(0,o.kt)("li",{parentName:"ol"},"Download and use a Natural Language Processing (NLP) pipeline that will tokenise, lemmatise, and Part Of Speech (POS) tag. In most cases this will be a spaCy pipeline. ",(0,o.kt)("strong",{parentName:"li"},"Note")," that the PyMUSAS ",(0,o.kt)("inlineCode",{parentName:"li"},"RuleBasedTagger")," only requires at minimum the data to be tokenised but having the lemma and POS tag will improve the accuracy of the tagging of the text."),(0,o.kt)("li",{parentName:"ol"},"Run the PyMUSAS ",(0,o.kt)("inlineCode",{parentName:"li"},"RuleBasedTagger"),"."),(0,o.kt)("li",{parentName:"ol"},"Extract token level linguistic information from the tagged text, which will include USAS semantic tags."),(0,o.kt)("li",{parentName:"ol"},"For Chinese, Italian, Portuguese, Spanish, and Welsh taggers which support Multi Word Expression (MWE) identification and tagging we will show how to extract this information from the tagged text as well.")),(0,o.kt)("h2",{id:"chinese"},"Chinese"),(0,o.kt)("details",null,(0,o.kt)("summary",null,"Expand"),(0,o.kt)("p",null,"First download both the ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/UCREL/pymusas-models/releases/tag/cmn_dual_upos2usas_contextual-0.3.0"},"Chinese PyMUSAS ",(0,o.kt)("inlineCode",{parentName:"a"},"RuleBasedTagger")," spaCy component")," and the ",(0,o.kt)("a",{parentName:"p",href:"https://spacy.io/models/zh"},"small Chinese spaCy model"),":"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"pip install https://github.com/UCREL/pymusas-models/releases/download/cmn_dual_upos2usas_contextual-0.3.0/cmn_dual_upos2usas_contextual-0.3.0-py3-none-any.whl\npython -m spacy download zh_core_web_sm\n")),(0,o.kt)("p",null,"Then create the tagger, in a Python script:"),(0,o.kt)("div",{className:"admonition admonition-note alert alert--secondary"},(0,o.kt)("div",{parentName:"div",className:"admonition-heading"},(0,o.kt)("h5",{parentName:"div"},(0,o.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,o.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,o.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"}))),"note")),(0,o.kt)("div",{parentName:"div",className:"admonition-content"},(0,o.kt)("p",{parentName:"div"},"Currently there is no lemmatisation component in the spaCy pipeline for Chinese."))),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"import spacy\n\n# We exclude the following components as we do not need them. \nnlp = spacy.load('zh_core_web_sm', exclude=['parser', 'ner'])\n# Load the Chinese PyMUSAS rule based tagger in a seperate spaCy pipeline\nchinese_tagger_pipeline = spacy.load('cmn_dual_upos2usas_contextual')\n# Adds the Chinese PyMUSAS rule based tagger to the main spaCy pipeline\nnlp.add_pipe('pymusas_rule_based_tagger', source=chinese_tagger_pipeline)\n")),(0,o.kt)("p",null,"The tagger is now setup for tagging text through the spaCy pipeline like so (this example follows on from the last). The example text is taken from the Chinese Wikipedia page on the topic of ",(0,o.kt)("a",{parentName:"p",href:"https://zh.wikipedia.org/wiki/%E5%B0%BC%E7%BD%97%E6%B2%B3"},(0,o.kt)("inlineCode",{parentName:"a"},"The Nile River")),":"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"text = \"\u5c3c\u7f57\u6cb3 \u662f\u4e00\u6761\u6d41\u7d93\u975e\u6d32\u6771\u90e8\u8207\u5317\u90e8\u7684\u6cb3\u6d41\uff0c\u8207\u4e2d\u975e\u5730\u5340\u7684\u525b\u679c\u6cb3\u3001\u975e\u6d32\u5357\u90e8\u7684\u8d5e\u6bd4\u897f\u6cb3\u4ee5\u53ca\u897f\u975e\u5730\u533a\u7684\u5c3c\u65e5\u5c14\u6cb3\u4e26\u5217\u975e\u6d32\u6700\u5927\u7684\u56db\u500b\u6cb3\u6d41\u7cfb\u7d71\u3002\"\n\noutput_doc = nlp(text)\n\nprint(f'Text\\tPOS\\tUSAS Tags')\nfor token in output_doc:\n    print(f'{token.text}\\t{token.pos_}\\t{token._.pymusas_tags}')\n")),(0,o.kt)("details",null,(0,o.kt)("summary",null,"Output:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-tsv"},"Text    POS     USAS Tags\n\u5c3c\u7f57\u6cb3     PROPN   ['Z2']\n\u662f       VERB    ['A3', 'Z5']\n\u4e00       NUM     ['N1']\n\u6761       NUM     ['G2.1/P1', 'S7.4-', 'A1.7+', 'S8-']\n\u6d41\u7d93      ADJ     ['Z99']\n\u975e\u6d32      PROPN   ['Z2']\n\u6771\u90e8      NOUN    ['Z99']\n\u8207\u5317\u90e8     PROPN   ['Z99']\n\u7684       PART    ['Z5']\n\u6cb3\u6d41      NOUN    ['W3/M4', 'N5+']\n\uff0c       PUNCT   ['PUNCT']\n\u8207       VERB    ['Z99']\n\u4e2d\u975e      PROPN   ['Z99']\n\u5730\u5340      NOUN    ['Z99']\n\u7684       PART    ['Z5']\n\u525b\u679c\u6cb3     PROPN   ['Z99']\n\u3001       PUNCT   ['PUNCT']\n\u975e\u6d32      PROPN   ['Z2']\n\u5357\u90e8      NOUN    ['M6']\n\u7684       PART    ['Z5']\n\u8d5e\u6bd4\u897f\u6cb3    NOUN    ['Z99']\n\u4ee5\u53ca      CCONJ   ['N5++', 'N5.2+', 'A13.3', 'Z5']\n\u897f\u975e      PROPN   ['Z99']\n\u5730\u533a      NOUN    ['A1.1.1', 'B3/X1', 'G1.1c', 'W3', 'F4/M7', 'K2', 'M7', 'A4.1', 'N3.6', 'B1', 'T1.1', 'O4.4', 'N5.1-', 'S5+c', 'B3', 'Y1', 'C1/H1@']\n\u7684       PART    ['Z5']\n\u5c3c\u65e5\u5c14\u6cb3    NOUN    ['Z99']\n\u4e26\u5217      VERB    ['Z99']\n\u975e\u6d32      PROPN   ['Z2']\n\u6700       ADV     ['A11.1+', 'N5+++', 'N3.2+++', 'A11.1+++', 'N5.1+', 'O2/M4', 'O3']\n\u5927       VERB    ['A11.1+', 'N5+++', 'N3.2+++', 'A11.1+++', 'N5.1+', 'O2/M4', 'O3']\n\u7684       PART    ['Z5']\n\u56db       NUM     ['N1']\n\u500b       NUM     ['N1']\n\u6cb3\u6d41      NOUN    ['W3/M4', 'N5+']\n\u7cfb\u7d71      NOUN    ['Z99']\n\u3002       PUNCT   ['PUNCT']\n"))),(0,o.kt)("p",null,"For Chinese the tagger also identifies and tags Multi Word Expressions (MWE), to find these MWE's you can run the following:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"print(f'Text\\tPOS\\tMWE start and end index\\tUSAS Tags')\nfor token in output_doc:\n    start, end = token._.pymusas_mwe_indexes[0]\n    if (end - start) > 1:\n        print(f'{token.text}\\t{token.pos_}\\t{(start, end)}\\t{token._.pymusas_tags}')\n")),(0,o.kt)("p",null,"Which will output the following:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-tsv"},"Text    POS    MWE start and end index    USAS Tags\n\u6700       ADV    (28, 30)                   ['A11.1+', 'N5+++', 'N3.2+++', 'A11.1+++', 'N5.1+', 'O2/M4', 'O3']\n\u5927       VERB   (28, 30)                   ['A11.1+', 'N5+++', 'N3.2+++', 'A11.1+++', 'N5.1+', 'O2/M4', 'O3']\n"))),(0,o.kt)("h2",{id:"dutch"},"Dutch"),(0,o.kt)("details",null,(0,o.kt)("summary",null,"Expand"),(0,o.kt)("p",null,"First download both the ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/UCREL/pymusas-models/releases/tag/nl_single_upos2usas_contextual-0.3.0"},"Dutch PyMUSAS ",(0,o.kt)("inlineCode",{parentName:"a"},"RuleBasedTagger")," spaCy component")," and the ",(0,o.kt)("a",{parentName:"p",href:"https://spacy.io/models/nl"},"small Dutch spaCy model"),":"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"pip install https://github.com/UCREL/pymusas-models/releases/download/nl_single_upos2usas_contextual-0.3.0/nl_single_upos2usas_contextual-0.3.0-py3-none-any.whl\npython -m spacy download nl_core_news_sm\n")),(0,o.kt)("p",null,"Then create the tagger, in a Python script:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"import spacy\n\n# We exclude the following components as we do not need them. \nnlp = spacy.load('nl_core_news_sm', exclude=['parser', 'ner', 'tagger'])\n# Load the Dutch PyMUSAS rule based tagger in a separate spaCy pipeline\ndutch_tagger_pipeline = spacy.load('nl_single_upos2usas_contextual')\n# Adds the Dutch PyMUSAS rule based tagger to the main spaCy pipeline\nnlp.add_pipe('pymusas_rule_based_tagger', source=dutch_tagger_pipeline)\n")),(0,o.kt)("p",null,"The tagger is now setup for tagging text through the spaCy pipeline like so (this example follows on from the last). The example text is taken from the Dutch Wikipedia page on the topic of ",(0,o.kt)("a",{parentName:"p",href:"https://nl.wikipedia.org/wiki/Nijl"},(0,o.kt)("inlineCode",{parentName:"a"},"The Nile River")),":"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"text = \"De Nijl is met een lengte van 5499 tot 6695 km de langste of de op een na langste rivier van de wereld.\"\n\noutput_doc = nlp(text)\n\nprint(f'Text\\tLemma\\tPOS\\tUSAS Tags')\nfor token in output_doc:\n    print(f'{token.text}\\t{token.lemma_}\\t{token.pos_}\\t{token._.pymusas_tags}')\n")),(0,o.kt)("details",null,(0,o.kt)("summary",null,"Output:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-tsv"},"Text    Lemma   POS     USAS Tags\nDe      de      DET     ['Z5']\nNijl    nijl    PROPN   ['Z99']\nis      is      AUX     ['Z99']\nmet     met     ADP     ['Z5']\neen     een     DET     ['Z5']\nlengte  lengte  NOUN    ['N3.7', 'T1.3', 'M4']\nvan     van     ADP     ['Z5']\n5499    5499    NUM     ['N1']\ntot     tot     ADP     ['Z99']\n6695    6695    NUM     ['N1']\nkm      km      SYM     ['Z99']\nde      de      DET     ['Z5']\nlangste lang    ADJ     ['N3.7+', 'T1.3+', 'N3.3+', 'N3.2+', 'X7+']\nof      of      CCONJ   ['Z5']\nde      de      DET     ['Z5']\nop      op      ADP     ['A5.1+', 'G2.2+', 'A1.1.1', 'M6', 'Z5']\neen     e       NUM     ['N1', 'T3', 'T1.2', 'Z8']\nna      na      ADP     ['N4', 'Z5']\nlangste lang    ADJ     ['N3.7+', 'T1.3+', 'N3.3+', 'N3.2+', 'X7+']\nrivier  rivier  NOUN    ['W3/M4', 'N5+']\nvan     van     ADP     ['Z5']\nde      de      DET     ['Z5']\nwereld  wereld  NOUN    ['W1', 'S5+c', 'A4.1', 'N5+']\n.       .       PUNCT   ['PUNCT']\n")))),(0,o.kt)("h2",{id:"french"},"French"),(0,o.kt)("details",null,(0,o.kt)("summary",null,"Expand"),(0,o.kt)("p",null,"First download both the ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/UCREL/pymusas-models/releases/tag/fr_single_upos2usas_contextual-0.3.0"},"French PyMUSAS ",(0,o.kt)("inlineCode",{parentName:"a"},"RuleBasedTagger")," spaCy component")," and the ",(0,o.kt)("a",{parentName:"p",href:"https://spacy.io/models/fr"},"small French spaCy model"),":"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"pip install https://github.com/UCREL/pymusas-models/releases/download/fr_single_upos2usas_contextual-0.3.0/fr_single_upos2usas_contextual-0.3.0-py3-none-any.whl\npython -m spacy download fr_core_news_sm\n")),(0,o.kt)("p",null,"Then create the tagger, in a Python script:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"import spacy\n\n# We exclude the following components as we do not need them. \nnlp = spacy.load('fr_core_news_sm', exclude=['parser', 'ner'])\n# Load the French PyMUSAS rule based tagger in a separate spaCy pipeline\nfrench_tagger_pipeline = spacy.load('fr_single_upos2usas_contextual')\n# Adds the French PyMUSAS rule based tagger to the main spaCy pipeline\nnlp.add_pipe('pymusas_rule_based_tagger', source=french_tagger_pipeline)\n")),(0,o.kt)("p",null,"The tagger is now setup for tagging text through the spaCy pipeline like so (this example follows on from the last). The example text is taken from the French Wikipedia page on the topic of ",(0,o.kt)("a",{parentName:"p",href:"https://fr.wikipedia.org/wiki/Nil"},(0,o.kt)("inlineCode",{parentName:"a"},"The Nile River")),":"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"text = \"Le Nil est un fleuve d'Afrique. Avec une longueur d'environ 6 700 km, c'est avec le fleuve Amazone, le plus long fleuve du monde.\"\n\noutput_doc = nlp(text)\n\nprint(f'Text\\tLemma\\tPOS\\tUSAS Tags')\nfor token in output_doc:\n    print(f'{token.text}\\t{token.lemma_}\\t{token.pos_}\\t{token._.pymusas_tags}')\n")),(0,o.kt)("details",null,(0,o.kt)("summary",null,"Output:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-tsv"},"Text      Lemma     POS       USAS Tags\nLe        le        DET       ['Z5']\nNil       Nil       PROPN     ['Z99']\nest       \xeatre      AUX       ['M6']\nun        un        DET       ['Z5']\nfleuve    fleuve    NOUN      ['W3/M4', 'N5+']\nd'        de        ADP       ['Z5']\nAfrique   Afrique   PROPN     ['Z99']\n.         .         PUNCT     ['PUNCT']\nAvec      avec      ADP       ['Z5']\nune       un        DET       ['Z5']\nlongueur  longueur  NOUN      ['N3.7', 'T1.3', 'M4']\nd'        de        ADP       ['Z5']\nenviron   environ   ADV       ['Z5']\n6         6         DET       ['Z99']\n700       700       NUM       ['N1']\nkm        kilom\xe8tre NOUN      ['N3.3', 'N3.7']\n,         ,         PUNCT     ['PUNCT']\nc'        ce        PRON      ['Z8']\nest       \xeatre      VERB      ['M6']\navec      avec      ADP       ['Z5']\nle        le        DET       ['Z5']\nfleuve    fleuve    NOUN      ['W3/M4', 'N5+']\nAmazone   amazone   NOUN      ['Z99']\n,         ,         PUNCT     ['PUNCT']\nle        le        DET       ['Z5']\nplus      plus      ADV       ['Z5']\nlong      long      ADJ       ['Z99']\nfleuve    fleuve    NOUN      ['W3/M4', 'N5+']\ndu        de        ADP       ['Z5']\nmonde     monde     NOUN      ['Z99']\n.         .         PUNCT     ['PUNCT']\n")))),(0,o.kt)("h2",{id:"italian"},"Italian"),(0,o.kt)("details",null,(0,o.kt)("summary",null,"Expand"),(0,o.kt)("p",null,"First download both the ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/UCREL/pymusas-models/releases/tag/it_dual_upos2usas_contextual-0.3.0"},"Italian PyMUSAS ",(0,o.kt)("inlineCode",{parentName:"a"},"RuleBasedTagger")," spaCy component")," and the ",(0,o.kt)("a",{parentName:"p",href:"https://spacy.io/models/it"},"small Italian spaCy model"),":"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"pip install https://github.com/UCREL/pymusas-models/releases/download/it_dual_upos2usas_contextual-0.3.0/it_dual_upos2usas_contextual-0.3.0-py3-none-any.whl\npython -m spacy download it_core_news_sm\n")),(0,o.kt)("p",null,"Then create the tagger, in a Python script:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"import spacy\n\n# We exclude the following components as we do not need them. \nnlp = spacy.load('it_core_news_sm', exclude=['parser', 'ner', 'tagger'])\n# Load the Italian PyMUSAS rule based tagger in a separate spaCy pipeline\nitalian_tagger_pipeline = spacy.load('it_dual_upos2usas_contextual')\n# Adds the Italian PyMUSAS rule based tagger to the main spaCy pipeline\nnlp.add_pipe('pymusas_rule_based_tagger', source=italian_tagger_pipeline)\n")),(0,o.kt)("p",null,"The tagger is now setup for tagging text through the spaCy pipeline like so (this example follows on from the last). The example text is taken from the Italian Wikipedia page on the topic of ",(0,o.kt)("a",{parentName:"p",href:"https://it.wikipedia.org/wiki/Nilo"},(0,o.kt)("inlineCode",{parentName:"a"},"The Nile River")),":"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"text = \"Il Nilo \xe8 un fiume africano lungo 6.852 km che attraversa otto stati dell'Africa. Tradizionalmente considerato il fiume pi\xf9 lungo del mondo, contende il primato della lunghezza al Rio delle Amazzoni.\"\n\noutput_doc = nlp(text)\n\nprint(f'Text\\tLemma\\tPOS\\tUSAS Tags')\nfor token in output_doc:\n    print(f'{token.text}\\t{token.lemma_}\\t{token.pos_}\\t{token._.pymusas_tags}')\n")),(0,o.kt)("details",null,(0,o.kt)("summary",null,"Output:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-tsv"},"Text              Lemma             POS     USAS Tags\nIl                il                DET     ['Z5']\nNilo              nilo              PROPN   ['Z99']\n\xe8                 essere            AUX     ['A5.1', 'S7.1++', 'X3.2', 'Q2.2', 'A8', 'N3.1%']\nun                uno               DET     ['Z5']\nfiume             fiume             NOUN    ['W3']\nafricano          africano          ADJ     ['Z2']\nlungo             lungo             ADP     ['Z5']\n6.852             6.852             NUM     ['N1']\nkm                km                NOUN    ['N3.3']\nche               che               PRON    ['Z8']\nattraversa        attraversare      VERB    ['M1', 'M6', 'S8-', 'A1.8+', 'A6.3+', 'F4/L2', 'O4.4', 'Q1.2', 'E3-', 'S1.1.1', 'S9@']\notto              otto              NUM     ['N1']\nstati             stato             NOUN    ['G2.1/H1', 'B2', 'A3']\ndell'             dell'             ADP     ['Z99']\nAfrica            Africa            PROPN   ['Z2']\n.                 .                 PUNCT   ['PUNCT']\nTradizionalmente  tradizionalmente  ADV     ['Z99']\nconsiderato       considerare       VERB    ['A5.1', 'N2', 'A11.1+', 'Q2.2', 'S1.1.1', 'Q1.3', 'S9%', 'X2.1', 'X2.4', 'X6']\nil                il                DET     ['Z5']\nfiume             fiume             NOUN    ['W3']\npi\xf9               molto             ADV     ['N3.3+', 'A13.3']\nlungo             lungo             ADJ     ['N3.3+', 'A13.3']\ndel               del               ADP     ['Z5']\nmondo             mondo             NOUN    ['W1']\n,                 ,                 PUNCT   ['PUNCT']\ncontende          contendere        VERB    ['S7.3']\nil                il                DET     ['Z5']\nprimato           primato           NOUN    ['A5.1+++', 'A11.1+']\ndella             della             ADP     ['Z99']\nlunghezza         lunghezza         NOUN    ['N3.7', 'T1.3', 'M4']\nal                al                ADP     ['Z5']\nRio               Rio               PROPN   ['Z2']\ndelle             della             ADP     ['Z5']\nAmazzoni          amazzoni          PROPN   ['Z99']\n.                 .                 PUNCT   ['PUNCT']\n"))),(0,o.kt)("p",null,"For Italian the tagger also identifies and tags Multi Word Expressions (MWE), to find these MWE's you can run the following:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"print(f'Text\\tPOS\\tMWE start and end index\\tUSAS Tags')\n\nfor token in output_doc:\n    start, end = token._.pymusas_mwe_indexes[0]\n    if (end - start) > 1:\n        print(f'{token.text}\\t{token.pos_}\\t{(start, end)}\\t{token._.pymusas_tags}')\n")),(0,o.kt)("p",null,"Which will output the following:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-tsv"},"Text    POS     MWE start and end index    USAS Tags\npi\xf9     ADV     (20, 22)                   ['N3.3+', 'A13.3']\nlungo   ADJ     (20, 22)                   ['N3.3+', 'A13.3']\n"))),(0,o.kt)("h2",{id:"portuguese"},"Portuguese"),(0,o.kt)("details",null,(0,o.kt)("summary",null,"Expand"),(0,o.kt)("p",null,"First download both the ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/UCREL/pymusas-models/releases/tag/pt_dual_upos2usas_contextual-0.3.0"},"Portuguese PyMUSAS ",(0,o.kt)("inlineCode",{parentName:"a"},"RuleBasedTagger")," spaCy component")," and the ",(0,o.kt)("a",{parentName:"p",href:"https://spacy.io/models/pt"},"small Portuguese spaCy model"),":"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"pip install https://github.com/UCREL/pymusas-models/releases/download/pt_dual_upos2usas_contextual-0.3.0/pt_dual_upos2usas_contextual-0.3.0-py3-none-any.whl\npython -m spacy download pt_core_news_sm\n")),(0,o.kt)("p",null,"Then create the tagger, in a Python script:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"import spacy\n\n# We exclude the following components as we do not need them. \nnlp = spacy.load('pt_core_news_sm', exclude=['parser', 'ner'])\n# Load the Portuguese PyMUSAS rule based tagger in a separate spaCy pipeline\nportuguese_tagger_pipeline = spacy.load('pt_dual_upos2usas_contextual')\n# Adds the Portuguese PyMUSAS rule based tagger to the main spaCy pipeline\nnlp.add_pipe('pymusas_rule_based_tagger', source=portuguese_tagger_pipeline)\n")),(0,o.kt)("p",null,"The tagger is now setup for tagging text through the spaCy pipeline like so (this example follows on from the last). The example text is taken from the Portuguese Wikipedia page on the topic of ",(0,o.kt)("a",{parentName:"p",href:"https://pt.wikipedia.org/wiki/Rio_Nilo"},(0,o.kt)("inlineCode",{parentName:"a"},"The Nile River")),":"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"text = \"Todos estes estudos levam a que o comprimento de ambos os rios permane\xe7a em aberto, continuando por isso o debate e como tal, continuando-se a considerar o Nilo como o rio mais longo.\"\n\noutput_doc = nlp(text)\n\nprint(f'Text\\tLemma\\tPOS\\tUSAS Tags')\nfor token in output_doc:\n    print(f'{token.text}\\t{token.lemma_}\\t{token.pos_}\\t{token._.pymusas_tags}')\n")),(0,o.kt)("details",null,(0,o.kt)("summary",null,"Output:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-tsv"},"Text            Lemma           POS     USAS Tags\nTodos           Todos           DET     ['Z8/N5.1+c']\nestes           este            DET     ['Z5', 'Z8']\nestudos         estudo          NOUN    ['P1', 'X2.4', 'H2', 'Q1.2', 'C1']\nlevam           levar           VERB    ['A9+', 'T1.3', 'C1', 'A1.1.1', 'M2', 'S7.1-', 'A2.1+', 'X2.4', 'S6+', 'S7.4+', 'N3', 'A2.1+', 'P1', 'M1', 'X2.5+', 'F1@', 'F2@', 'Q1.2@', 'B3@']\na               o               SCONJ   ['M6', 'Z5']\nque             que             SCONJ   ['A13.3', 'A6.1+', 'Z5', 'Z8']\no               o               DET     ['Z5']\ncomprimento     comprimento     NOUN    ['N3.7', 'T1.3', 'M4']\nde              de              ADP     ['Z5']\nambos           ambos           DET     ['N5']\nos              o               DET     ['Z5']\nrios            rio             NOUN    ['W3/M4', 'N5+']\npermane\xe7a       permanecer      VERB    ['T2++', 'M8', 'N5.2+']\nem              em              SCONJ   ['A5.1+', 'G2.2+', 'A1.1.1', 'M6', 'O4.2+', 'Z5']\naberto          aberto          VERB    ['A10+', 'T2+']\n,               ,               PUNCT   ['PUNCT']\ncontinuando     continuar       VERB    ['Z99']\npor             por             ADP     ['N4', 'Z5', 'T1.2']\nisso            isso            PRON    ['N4', 'Z5', 'T1.2']\no               o               DET     ['Z5']\ndebate          debater         NOUN    ['Q2.1', 'Q2.1/A6.1-', 'Q2.1/E3-', 'Q2.2']\ne               e               CCONJ   ['Z5']\ncomo            comer           ADP     ['Z5']\ntal             tal             PRON    ['Z5']\n,               ,               PUNCT   ['PUNCT']\ncontinuando-se  continuando-se  VERB    ['Z99']\na               o               SCONJ   ['M6', 'Z5']\nconsiderar      considerar      VERB    ['Z99']\no               o               DET     ['Z5']\nNilo            Nilo            PROPN   ['Z2']\ncomo            comer           ADP     ['Z5']\no               o               DET     ['Z5']\nrio             rir             NOUN    ['W3/M4', 'N5+']\nmais            mais            ADV     ['T1.3++', 'N3.7++', 'N3.3++', 'N3.2++']\nlongo           longo           ADJ     ['T1.3++', 'N3.7++', 'N3.3++', 'N3.2++']\n.               .               PUNCT   ['PUNCT']\n"))),(0,o.kt)("p",null,"For Portuguese the tagger also identifies and tags Multi Word Expressions (MWE), to find these MWE's you can run the following:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"print(f'Text\\tPOS\\tMWE start and end index\\tUSAS Tags')\n\nfor token in output_doc:\n    start, end = token._.pymusas_mwe_indexes[0]\n    if (end - start) > 1:\n        print(f'{token.text}\\t{token.pos_}\\t{(start, end)}\\t{token._.pymusas_tags}')\n")),(0,o.kt)("p",null,"Which will output the following:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-tsv"},"Text    POS     MWE start and end index    USAS Tags\npor     ADP     (17, 19)                   ['N4', 'Z5', 'T1.2']\nisso    PRON    (17, 19)                   ['N4', 'Z5', 'T1.2']\nmais    ADV     (33, 35)                   ['T1.3++', 'N3.7++', 'N3.3++', 'N3.2++']\nlongo   ADJ     (33, 35)                   ['T1.3++', 'N3.7++', 'N3.3++', 'N3.2++']\n"))),(0,o.kt)("h2",{id:"spanish"},"Spanish"),(0,o.kt)("details",null,(0,o.kt)("summary",null,"Expand"),(0,o.kt)("p",null,"First download both the ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/UCREL/pymusas-models/releases/tag/es_dual_upos2usas_contextual-0.3.0"},"Spanish PyMUSAS ",(0,o.kt)("inlineCode",{parentName:"a"},"RuleBasedTagger")," spaCy component")," and the ",(0,o.kt)("a",{parentName:"p",href:"https://spacy.io/models/es"},"small Spanish spaCy model"),":"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"pip install https://github.com/UCREL/pymusas-models/releases/download/es_dual_upos2usas_contextual-0.3.0/es_dual_upos2usas_contextual-0.3.0-py3-none-any.whl\npython -m spacy download es_core_news_sm\n")),(0,o.kt)("p",null,"Then create the tagger, in a Python script:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"import spacy\n\n# We exclude the following components as we do not need them. \nnlp = spacy.load('es_core_news_sm', exclude=['parser', 'ner'])\n# Load the Spanish PyMUSAS rule based tagger in a separate spaCy pipeline\nspanish_tagger_pipeline = spacy.load('es_dual_upos2usas_contextual')\n# Adds the Spanish PyMUSAS rule based tagger to the main spaCy pipeline\nnlp.add_pipe('pymusas_rule_based_tagger', source=spanish_tagger_pipeline)\n")),(0,o.kt)("p",null,"The tagger is now setup for tagging text through the spaCy pipeline like so (this example follows on from the last). The example text is taken from the Spanish Wikipedia page on the topic of ",(0,o.kt)("a",{parentName:"p",href:"https://es.wikipedia.org/wiki/Pa%C3%ADses_Bajos"},(0,o.kt)("inlineCode",{parentName:"a"},"Pa\xedses Bajos")),":"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"text = \"Los Pa\xedses Bajos son un pa\xeds soberano ubicado al noreste de la Europa continental y el pa\xeds constituyente m\xe1s grande de los cuatro que, junto con las islas de Aruba, Curazao y San Mart\xedn, forman el Reino de los Pa\xedses Bajos.\"\n\noutput_doc = nlp(text)\n\nprint(f'Text\\tLemma\\tPOS\\tUSAS Tags')\nfor token in output_doc:\n    print(f'{token.text}\\t{token.lemma_}\\t{token.pos_}\\t{token._.pymusas_tags}')\n")),(0,o.kt)("details",null,(0,o.kt)("summary",null,"Output:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-tsv"},"Text            Lemma           POS     USAS Tags\nLos             el              DET     ['Z5']\nPa\xedses          Pa\xedses          PROPN   ['Z2']\nBajos           Bajos           PROPN   ['Z2']\nson             ser             AUX     ['A3+', 'L1', 'Z5']\nun              uno             DET     ['Z5', 'N1']\npa\xeds            pa\xeds            NOUN    ['G1.1c', 'W3', 'M7']\nsoberano        soberano        ADJ     ['Z99']\nubicado         ubicado         ADJ     ['Z99']\nal              al              ADP     ['Z5']\nnoreste         noreste         NOUN    ['Z99']\nde              de              ADP     ['Z5']\nla              el              DET     ['Z5']\nEuropa          Europa          PROPN   ['Z2', 'S7', 'M7']\ncontinental     continental     ADJ     ['Z99']\ny               y               CCONJ   ['Z5', 'A1.8+']\nel              el              DET     ['Z5']\npa\xeds            pa\xeds            NOUN    ['G1.1c', 'W3', 'M7']\nconstituyente   constituyente   ADJ     ['Z99']\nm\xe1s             m\xe1s             ADV     ['A13.3', 'N6++', 'Z5']\ngrande          grande          ADJ     ['N3.1+/A6.1+/A13.2+', 'A5']\nde              de              ADP     ['Z5']\nlos             el              DET     ['Z5']\ncuatro          cuatro          NUM     ['N1']\nque             que             PRON    ['Z5', 'Z8']\n,               ,               PUNCT   ['PUNCT']\njunto           junto           ADJ     ['A2.2', 'S5+', 'A1.8+']\ncon             con             ADP     ['Z5', 'A4.1']\nlas             el              DET     ['Z5']\nislas           isla            NOUN    ['W3M7']\nde              de              ADP     ['Z5']\nAruba           Aruba           PROPN   ['Z99']\n,               ,               PUNCT   ['PUNCT']\nCurazao         Curazao         PROPN   ['Z99']\ny               y               CCONJ   ['Z5', 'A1.8+']\nSan             San             PROPN   ['S9', 'S2', 'A4.1']\nMart\xedn          Mart\xedn          PROPN   ['Z1', 'S2']\n,               ,               PUNCT   ['PUNCT']\nforman          formar          VERB    ['T2+', 'A2.1+', 'A1.8+', 'A3+', 'A1.1.1']\nel              el              DET     ['Z5']\nReino           Reino           PROPN   ['M7']\nde              de              ADP     ['Z5']\nlos             el              DET     ['Z5']\nPa\xedses          Pa\xedses          PROPN   ['Z2']\nBajos           Bajos           PROPN   ['Z2']\n.               .               PUNCT   ['PUNCT']\n"))),(0,o.kt)("p",null,"For Spainsh the tagger also identifies and tags Multi Word Expressions (MWE), to find these MWE's you can run the following:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"print(f'Text\\tPOS\\tMWE start and end index\\tUSAS Tags')\n\nfor token in output_doc:\n    start, end = token._.pymusas_mwe_indexes[0]\n    if (end - start) > 1:\n        print(f'{token.text}\\t{token.pos_}\\t{(start, end)}\\t{token._.pymusas_tags}')\n")),(0,o.kt)("p",null,"Which will output the following:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-tsv"},"Text    POS     MWE start and end index    USAS Tags\nPa\xedses  PROPN   (1, 3)                     ['Z2']\nBajos   PROPN   (1, 3)                     ['Z2']\nPa\xedses  PROPN   (42, 44)                   ['Z2']\nBajos   PROPN   (42, 44)                   ['Z2']\n"))),(0,o.kt)("h2",{id:"welsh"},"Welsh"),(0,o.kt)("details",null,(0,o.kt)("summary",null,"Expand"),(0,o.kt)("p",null,"In this example we will not be using spaCy for tokenisation, lemmatisation, and POS tagging, as we will be using the ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/UCREL/CyTag"},"CyTag toolkit")," that has been wrapped in a docker container. Therefore, first you will need to ",(0,o.kt)("a",{parentName:"p",href:"https://docs.docker.com/get-docker/"},"install docker"),"."),(0,o.kt)("p",null,"We assume that you would like to tag the following text, of which this text is stored in the file named ",(0,o.kt)("inlineCode",{parentName:"p"},"welsh_text_example.txt"),". The example text is taken from the Welsh Wikipedia page on the topic of ",(0,o.kt)("a",{parentName:"p",href:"https://cy.wikipedia.org/wiki/Banc"},(0,o.kt)("inlineCode",{parentName:"a"},"Bank")," as a financial institution.")," With an additional random sentence at the end to demonstrate the Multi Word Expression (MWE) identification and tagging attributes of the tagger."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-txt",metastring:'title="welsh_text_example.txt"',title:'"welsh_text_example.txt"'},"Sefydliad cyllidol yw bancwr neu fanc sy'n actio fel asiant talu ar gyfer cwsmeriaid, ac yn rhoi benthyg ac yn benthyg arian. Yn rhai gwledydd, megis yr Almaen a Siapan, mae banciau'n brif berchenogion corfforaethau diwydiannol, tra mewn gwledydd eraill, megis yr Unol Daleithiau, mae banciau'n cael eu gwahardd rhag bod yn berchen ar gwmniau sydd ddim yn rhai cyllidol. Adran Iechyd Cymru.\n")),(0,o.kt)("p",null,"First we will need to run the CyTag toolkit, more specifically we will run version 1 of the toolkit as we have a mapping from the POS tags produced in version 1 (the ",(0,o.kt)("a",{parentName:"p",href:"https://cytag.corcencc.org/tagset?lang=en"},"basic CorCencC POS tagset"),") to the POS tags that the USAS lexicon uses (the USAS core POS tagset) within the pre-configured Welsh PyMUSAS ",(0,o.kt)("inlineCode",{parentName:"p"},"RuleBasedTagger")," tagger."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"cat welsh_text_example.txt | docker run -i --rm ghcr.io/ucrel/cytag:1.0.4 > welsh_text_example.tsv\n")),(0,o.kt)("p",null,"We now have a ",(0,o.kt)("inlineCode",{parentName:"p"},"tsv")," version of the file that has been tokenised, lemmatised, and POS tagged. The ",(0,o.kt)("inlineCode",{parentName:"p"},"welsh_text_example.tsv"),' file should contain the following (I have added column headers here to explain what each column represents, these headers should not be in your file, also note that the "Mutation" column is optional):'),(0,o.kt)("details",null,(0,o.kt)("summary",null,"welsh_text_example.tsv:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-tsv",metastring:'title="welsh_text_example.tsv"',title:'"welsh_text_example.tsv"'},"Line Number Token   Sentence Index, Token Index Lemma   Basic POS   Enriched POS    Mutation\n1   Sefydliad   1,1 sefydliad   E   Egu \n2   cyllidol    1,2 cyllidol    Ans Anscadu \n3   yw  1,3 bod B   Bpres3u \n4   bancwr  1,4 bancwr  E   Egu \n5   neu 1,5 neu Cys Cyscyd  \n6   fanc    1,6 banc    E   Egu +sm\n7   sy  1,7 bod B   Bpres3perth \n8   'n  1,8 yn  U   Uberf   \n9   actio   1,9 actio   B   Be  \n10  fel 1,10    fel Cys Cyscyd  \n11  asiant  1,11    asiant | asio   E | B   Egu | Bpres3ll  \n12  talu    1,12    talu    B   Be  \n13  ar  1,13    ar  Ar  Arsym   \n14  gyfer   1,14    cyfer   E   Egu +sm\n15  cwsmeriaid  1,15    cwsmer  E   Egll    \n16  ,   1,16    ,   Atd Atdcan  \n17  ac  1,17    a   Cys Cyscyd  \n18  yn  1,18    yn  U   Uberf   \n19  rhoi    1,19    rhoi    B   Be  \n20  benthyg 1,20    benthyg E   Egu \n21  ac  1,21    a   Cys Cyscyd  \n22  yn  1,22    yn  U   Uberf   \n23  benthyg 1,23    benthyg B   Be  \n24  arian   1,24    arian   E   Egu \n25  .   1,25    .   Atd Atdt    \n26  Yn  2,1 yn  Ar  Arsym   \n27  rhai    2,2 rhai    unk unk \n28  gwledydd    2,3 gwlad   E   Ebll    \n29  ,   2,4 ,   Atd Atdcan  \n30  megis   2,5 megis   Cys Cyscyd  \n31  yr  2,6 y   YFB YFB \n32  Almaen  2,7 Almaen  E   Epb \n33  a   2,8 a   Cys Cyscyd  \n34  Siapan  2,9 Siapan  E   Epb \n35  ,   2,10    ,   Atd Atdcan  \n36  mae 2,11    bod B   Bpres3u \n37  banciau 2,12    banc    E   Egll    \n38  'n  2,13    yn  U   Utra    \n39  brif    2,14    brif    unk unk \n40  berchenogion    2,15    berchenogion    unk unk \n41  corfforaethau   2,16    corfforaeth E   Ebll    \n42  diwydiannol 2,17    diwydiannol Ans Anscadu \n43  ,   2,18    ,   Atd Atdcan  \n44  tra 2,19    tra Cys Cyscyd  \n45  mewn    2,20    mewn    Ar  Arsym   \n46  gwledydd    2,21    gwlad   E   Ebll    \n47  eraill  2,22    arall   Ans Anscadu \n48  ,   2,23    ,   Atd Atdcan  \n49  megis   2,24    megis   Cys Cyscyd  \n50  yr  2,25    y   YFB YFB \n51  Unol    2,26    unol    Ans Anscadu \n52  Daleithiau  2,27    Daleithiau  E   Ep  \n53  ,   2,28    ,   Atd Atdcan  \n54  mae 2,29    bod B   Bpres3u \n55  banciau 2,30    banc    E   Egll    \n56  'n  2,31    yn  U   Uberf   \n57  cael    2,32    cael    B   Be  \n58  eu  2,33    eu  Rha Rhadib3ll   \n59  gwahardd    2,34    gwahardd    B   Be  \n60  rhag    2,35    rhag    Ar  Arsym   \n61  bod 2,36    bod B   Be  \n62  yn  2,37    yn  U   Utra    \n63  berchen 2,38    perchen E   Egu +sm\n64  ar  2,39    ar  Ar  Arsym   \n65  gwmniau 2,40    gwmniau unk unk \n66  sydd    2,41    bod B   Bpres3perth \n67  ddim    2,42    dim E   Egu +sm\n68  yn  2,43    yn  U   Utra    \n69  rhai    2,44    rhai    unk unk \n70  cyllidol    2,45    cyllidol    Ans Anscadu \n71  .   2,46    .   Atd Atdt    \n72  Adran   3,1 adran   E   Ebu \n73  Iechyd  3,2 iechyd  E   Egu \n74  Cymru   3,3 Cymru   E   Epb \n75  .   3,4 .   Atd Atdt\n"))),(0,o.kt)("p",null,"Now we have the token, lemma, and POS tag information we can run the ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/UCREL/pymusas-models/releases/tag/cy_dual_basiccorcencc2usas_contextual-0.3.0"},"Welsh PyMUSAS ",(0,o.kt)("inlineCode",{parentName:"a"},"RuleBasedTagger")),", so first we will download it:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"pip install https://github.com/UCREL/pymusas-models/releases/download/cy_dual_basiccorcencc2usas_contextual-0.3.0/cy_dual_basiccorcencc2usas_contextual-0.3.0-py3-none-any.whl\n")),(0,o.kt)("p",null,"Now we can run the tagger over the ",(0,o.kt)("inlineCode",{parentName:"p"},"tsv")," data using the following Python script:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"from pathlib import Path\nfrom typing import List\n\nimport spacy\nfrom spacy.tokens import Doc\nfrom spacy.vocab import Vocab\n\n# Load the Welsh PyMUSAS rule based tagger\nnlp = spacy.load(\"cy_dual_basiccorcencc2usas_contextual\")\n\ntokens: List[str] = []\nspaces: List[bool] = []\nbasic_pos_tags: List[str] = []\nlemmas: List[str] = []\n\nwelsh_tagged_file = Path(Path.cwd(), 'welsh_text_example.tsv').resolve()\n\nprint('Text\\tLemma\\tPOS\\tUSAS Tags')\nwith welsh_tagged_file.open('r', encoding='utf-8') as welsh_tagged_data:\n    for line in welsh_tagged_data:\n        line = line.strip()\n        if line:\n            line_tags = line.split('\\t')\n            tokens.append(line_tags[1])\n            lemmas.append(line_tags[3])\n            basic_pos_tags.append(line_tags[4])\n            spaces.append(True)\n\n\n# As the tagger is a spaCy component that expects tokens, pos, and lemma\n# we need to create a spaCy Doc object that will contain this information\ndoc = Doc(Vocab(), words=tokens, tags=basic_pos_tags, lemmas=lemmas)\noutput_doc = nlp(doc)\n\nprint(f'Text\\tLemma\\tPOS\\tUSAS Tags')\nfor token in output_doc:\n    print(f'{token.text}\\t{token.lemma_}\\t{token.tag_}\\t{token._.pymusas_tags}')\n")),(0,o.kt)("details",null,(0,o.kt)("summary",null,"Output:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-tsv"},"Text            Lemma           POS     USAS Tags\nSefydliad       sefydliad       E       ['S5+c', 'S7.1+', 'H1c', 'S1.1.1', 'T2+']\ncyllidol        cyllidol        Ans     ['I1']\nyw              bod             B       ['A3+', 'Z5']\nbancwr          bancwr          E       ['Z99']\nneu             neu             Cys     ['Z5']\nfanc            banc            E       ['I1.1', 'X2.6+', 'M1']\nsy              bod             B       ['A3+', 'Z5']\n'n              yn              U       ['Z5']\nactio           actio           B       ['A1.1.1', 'T1.1.2', 'A8', 'K4']\nfel             fel             Cys     ['Z5']\nasiant          asiant | asio   E | B   ['I2.1/S2mf', 'G3/S2mf', 'K4/S2mf']\ntalu            talu            B       ['I1.2', 'A9-', 'I1.1/I3.1']\nar              ar              Ar      ['Z5']\ngyfer           cyfer           E       ['M6', 'Q2.2', 'Q2.2', 'S7.1+', 'X4.2', 'K4']\ncwsmeriaid      cwsmer          E       ['I2.2/S2mf']\n,               ,               Atd     ['PUNCT']\nac              a               Cys     ['Z5']\nyn              yn              U       ['Z5']\nrhoi            rhoi            B       ['A9-', 'A1.1.1']\nbenthyg         benthyg         E       ['A9-']\nac              a               Cys     ['Z5']\nyn              yn              U       ['Z5']\nbenthyg         benthyg         B       ['A9-']\narian           arian           E       ['I1']\n.               .               Atd     ['PUNCT']\nYn              yn              Ar      ['Z5']\nrhai            rhai            unk     ['A13.5']\ngwledydd        gwlad           E       ['M7']\n,               ,               Atd     ['PUNCT']\nmegis           megis           Cys     ['Z5']\nyr              y               YFB     ['Z5']\nAlmaen          Almaen          E       ['Z2']\na               a               Cys     ['Z5']\nSiapan          Siapan          E       ['Z2']\n,               ,               Atd     ['PUNCT']\nmae             bod             B       ['A3+', 'Z5']\nbanciau         banc            E       ['I1.1', 'X2.6+', 'M1']\n'n              yn              U       ['Z5']\nbrif            brif            unk     ['Z99']\nberchenogion    berchenogion    unk     ['Z99']\ncorfforaethau   corfforaeth     E       ['I2.1/S5c', 'G1.1c']\ndiwydiannol     diwydiannol     Ans     ['I4']\n,               ,               Atd     ['PUNCT']\ntra             tra             Cys     ['Z5']\nmewn            mewn            Ar      ['Z5']\ngwledydd        gwlad           E       ['M7']\neraill          arall           Ans     ['A6.1-/Z8']\n,               ,               Atd     ['PUNCT']\nmegis           megis           Cys     ['Z5']\nyr              y               YFB     ['Z5']\nUnol            unol            Ans     ['S5+', 'A1.1.1']\nDaleithiau      Daleithiau      E       ['Z99']\n,               ,               Atd     ['PUNCT']\nmae             bod             B       ['A3+', 'Z5']\nbanciau         banc            E       ['I1.1', 'X2.6+', 'M1']\n'n              yn              U       ['Z5']\ncael            cael            B       ['A9+', 'Z5', 'X9.2+', 'A2.1+', 'A2.2', 'M1', 'M2', 'X2.5+', 'E4.1-']\neu              eu              Rha     ['Z8']\ngwahardd        gwahardd        B       ['S7.4-']\nrhag            rhag            Ar      ['Z5']\nbod             bod             B       ['A3+', 'Z5']\nyn              yn              U       ['Z5']\nberchen         perchen         E       ['A9+/S2mf']\nar              ar              Ar      ['Z5']\ngwmniau         gwmniau         unk     ['Z99']\nsydd            bod             B       ['A3+', 'Z5']\nddim            dim             E       ['Z6/Z8']\nyn              yn              U       ['Z5']\nrhai            rhai            unk     ['A13.5']\ncyllidol        cyllidol        Ans     ['I1']\n.               .               Atd     ['PUNCT']\nAdran           adran           E       ['G1.1']\nIechyd          iechyd          E       ['G1.1']\nCymru           Cymru           E       ['Z2', 'Z1mf']\n.               .               Atd     ['PUNCT']\n"))),(0,o.kt)("p",null,"For Welsh the tagger also identifies and tags Multi Word Expressions (MWE), to find these MWE's you can run the following:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"print(f'Text\\tPOS\\tMWE start and end index\\tUSAS Tags')\nfor token in output_doc:\n    start, end = token._.pymusas_mwe_indexes[0]\n    if (end - start) > 1:\n        print(f'{token.text}\\t{token.tag_}\\t{(start, end)}\\t{token._.pymusas_tags}')\n")),(0,o.kt)("p",null,"Which will output the following:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-tsv"},"Text    POS     MWE start and end index    USAS Tags\nAdran   E       (71, 73)                   ['G1.1']\nIechyd  E       (71, 73)                   ['G1.1']\n"))),(0,o.kt)("h2",{id:"indonesian"},"Indonesian"),(0,o.kt)("details",null,(0,o.kt)("summary",null,"Expand"),(0,o.kt)("p",null,"In this example we will not be using spaCy for tokenisation, lemmatisation, and POS tagging, as we will be using the ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/UCREL/Indonesian-TreeTagger-Docker-Build"},"Indonesian TreeTagger")," that has been wrapped in a docker container. Therefore, first you will need to ",(0,o.kt)("a",{parentName:"p",href:"https://docs.docker.com/get-docker/"},"install docker"),". After installing docker you will need to build the Indonesian TreeTagger docker container locally, of which by doing this you agree to the ",(0,o.kt)("a",{parentName:"p",href:"https://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/Tagger-Licence"},"TreeTagger license")," (this license stops you from re-distributing the TreeTagger code, therefore please do not upload your built docker container to a registry like ",(0,o.kt)("a",{parentName:"p",href:"https://hub.docker.com/"},"Docker Hub"),"), like so (docker container size roughly 139MB):"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"docker build -t indonesian-treetagger:1.0.0 https://github.com/UCREL/Indonesian-TreeTagger-Docker-Build.git#main\n")),(0,o.kt)("p",null,"We assume that you would like to tag the following text, of which this text is stored in the file named ",(0,o.kt)("inlineCode",{parentName:"p"},"indonesian_text_example.txt"),". The example text is taken from the Indonesian Wikipedia page on the topic of ",(0,o.kt)("a",{parentName:"p",href:"https://id.wikipedia.org/wiki/Bank"},(0,o.kt)("inlineCode",{parentName:"a"},"Bank")," as a financial institution.")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-txt",metastring:'title="indonesian_text_example.txt"',title:'"indonesian_text_example.txt"'},"Bank adalah sebuah lembaga keuangan intermediasi yang umumnya didirikan dengan kewenangan untuk menerima simpanan uang, meminjamkan uang, dan menerbitkan surat sanggup bayar.\n")),(0,o.kt)("p",null,"First we will need to run the Indonesian TreeTagger:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"cat indonesian_text_example.txt | docker run -i --rm indonesian-treetagger:1.0.0 > indonesian_text_example.tsv\n")),(0,o.kt)("p",null,"We now have a ",(0,o.kt)("inlineCode",{parentName:"p"},"tsv")," version of the file that has been tokenised, lemmatised, and POS tagged. The ",(0,o.kt)("inlineCode",{parentName:"p"},"indonesian_text_example.tsv")," file should contain the following (I have added column headers here to explain what each column represents, these headers should not be in your file):"),(0,o.kt)("div",{className:"admonition admonition-note alert alert--secondary"},(0,o.kt)("div",{parentName:"div",className:"admonition-heading"},(0,o.kt)("h5",{parentName:"div"},(0,o.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,o.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,o.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"}))),"note")),(0,o.kt)("div",{parentName:"div",className:"admonition-content"},(0,o.kt)("p",{parentName:"div"},"The POS tagset for Indonesian is not the USAS core or ",(0,o.kt)("a",{parentName:"p",href:"https://universaldependencies.org/u/pos/"},"UPOS")," tagset, but rather the ",(0,o.kt)("a",{parentName:"p",href:"https://drive.google.com/file/d/1Pnhj2vVEEP5eIc655Af-WPDXxthyZdwb/view"},"UI tagset"),"."))),(0,o.kt)("details",null,(0,o.kt)("summary",null,"indonesian_text_example.tsv:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-tsv",metastring:'title="indonesian_text_example.tsv"',title:'"indonesian_text_example.tsv"'},"Token   POS Lemma\nBank    NNP bank\nadalah  VB  adalah\nsebuah  NND sebuah\nlembaga keuangan    NN  lembaga\nintermediasi    NN  intermediasi\nyang    SC  yang\numumnya NN  umumnya\ndidirikan   VB  diri\ndengan  IN  dengan\nkewenangan  NN  wenang\nuntuk   SC  untuk\nmenerima    VB  terima\nsimpanan    NN  simpan\nuang    NN  uang\n,   Z   ,\nmeminjamkan VB  pinjam\nuang    NN  uang\n,   Z   ,\ndan CC  dan\nmenerbitkan VB  terbit\nsurat   NN  surat\nsanggup VB  sanggup\nbayar   VB  bayar\n.   Z   .\n"))),(0,o.kt)("p",null,"Now we have the token, lemma, and POS tag information we can run the ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/UCREL/pymusas-models/releases/tag/id_single_none_contextual-0.3.0"},"Indonsian PyMUSAS ",(0,o.kt)("inlineCode",{parentName:"a"},"RuleBasedTagger")),", so first we will download it:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"pip install https://github.com/UCREL/pymusas-models/releases/download/id_single_none_contextual-0.3.0/id_single_none_contextual-0.3.0-py3-none-any.whl\n")),(0,o.kt)("p",null,"Now we can run the tagger over the ",(0,o.kt)("inlineCode",{parentName:"p"},"tsv")," data using the following Python script:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"from pathlib import Path\nfrom typing import List\n\nimport spacy\nfrom spacy.tokens import Doc\nfrom spacy.vocab import Vocab\n\n# Load the Indonesian PyMUSAS rule based tagger\nnlp = spacy.load(\"id_single_none_contextual\")\n\ntokens: List[str] = []\nspaces: List[bool] = []\npos_tags: List[str] = []\nlemmas: List[str] = []\n\nindonesian_tagged_file = Path(Path.cwd(), 'indonesian_text_example.tsv').resolve()\n\nprint('Text\\tLemma\\tPOS\\tUSAS Tags')\nwith indonesian_tagged_file.open('r', encoding='utf-8') as indonesian_tagged_data:\n    for line in indonesian_tagged_data:\n        line = line.strip()\n        if line:\n            line_tags = line.split('\\t')\n            tokens.append(line_tags[0])\n            lemmas.append(line_tags[2])\n            pos_tags.append(line_tags[1])\n            spaces.append(True)\n\n\n# As the tagger is a spaCy component that expects tokens, pos, and lemma\n# we need to create a spaCy Doc object that will contain this information\ndoc = Doc(Vocab(), words=tokens, tags=pos_tags, lemmas=lemmas)\noutput_doc = nlp(doc)\n\nprint(f'Text\\tLemma\\tPOS\\tUSAS Tags')\nfor token in output_doc:\n    print(f'{token.text}\\t{token.lemma_}\\t{token.tag_}\\t{token._.pymusas_tags}')\n")),(0,o.kt)("details",null,(0,o.kt)("summary",null,"Output:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-tsv"},"Text                Lemma               POS     USAS Tags\nBank                bank                NNP     ['Z99']\nadalah              adalah              VB      ['Z99']\nsebuah              sebuah              NND     ['Z99']\nlembaga keuangan    lembaga             NN      ['Z99']\nintermediasi        intermediasi        NN      ['Z99']\nyang                yang                SC      ['Z5']\numumnya             umumnya             NN      ['Z99']\ndidirikan           diri                VB      ['Z99']\ndengan              dengan              IN      ['Z5']\nkewenangan          wenang              NN      ['Z99']\nuntuk               untuk               SC      ['Z5']\nmenerima            terima              VB      ['Z99']\nsimpanan            simpan              NN      ['Z99']\nuang                uang                NN      ['Z99']\n,                   ,                   Z       ['PUNCT']\nmeminjamkan         pinjam              VB      ['Z99']\nuang                uang                NN      ['Z99']\n,                   ,                   Z       ['PUNCT']\ndan                 dan                 CC      ['Z5']\nmenerbitkan         terbit              VB      ['Z99']\nsurat               surat               NN      ['Z99']\nsanggup             sanggup             VB      ['Z99']\nbayar               bayar               VB      ['Z99']\n.                   .                   Z       ['PUNCT']\n")))))}m.isMDXComponent=!0}}]);